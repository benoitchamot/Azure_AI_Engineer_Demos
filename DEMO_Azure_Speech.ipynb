{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install azure-identity\n",
    "# pip install azure-keyvault-secrets\n",
    "# !pip install azure-ai-textanalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azure.keyvault.secrets import SecretClient\n",
    "# from azure.identity import DefaultAzureCredential\n",
    "from azure.core.credentials import AzureKeyCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Azure_Utils import secrets_get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Retrieve key\n",
    "secretName = os.environ[\"SPEECH_SECRET\"]\n",
    "service_key = secrets_get(secretName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Azure AI Speech service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=service_key, region=\"australiaeast\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text-to-Speech\n",
    "- [Quickstart: Recognize and convert speech to text](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/get-started-speech-to-text?tabs=windows%2Cterminal&pivots=programming-language-python)\n",
    "- [What is text to speech?](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/text-to-speech)\n",
    "- [Example notebook (GitHub)](https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/quickstart/python/text-to-speech/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthesizer with the given settings.\n",
    "# Since no explicit audio config is specified, the default speaker\n",
    "# will be used (make sure the audio settings are correct).\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en-AU-NatashaNeural\n",
      "en-AU-WilliamNeural\n",
      "en-AU-AnnetteNeural\n",
      "en-AU-CarlyNeural\n",
      "en-AU-DarrenNeural\n",
      "en-AU-DuncanNeural\n",
      "en-AU-ElsieNeural\n",
      "en-AU-FreyaNeural\n",
      "en-AU-JoanneNeural\n",
      "en-AU-KenNeural\n",
      "en-AU-KimNeural\n",
      "en-AU-NeilNeural\n",
      "en-AU-TimNeural\n",
      "en-AU-TinaNeural\n",
      "en-CA-ClaraNeural\n",
      "en-CA-LiamNeural\n",
      "en-GB-SoniaNeural\n",
      "en-GB-RyanNeural\n",
      "en-GB-LibbyNeural\n",
      "en-GB-AdaMultilingualNeural\n",
      "en-GB-OllieMultilingualNeural\n",
      "en-GB-AbbiNeural\n",
      "en-GB-AlfieNeural\n",
      "en-GB-BellaNeural\n",
      "en-GB-ElliotNeural\n",
      "en-GB-EthanNeural\n",
      "en-GB-HollieNeural\n",
      "en-GB-MaisieNeural\n",
      "en-GB-NoahNeural\n",
      "en-GB-OliverNeural\n",
      "en-GB-OliviaNeural\n",
      "en-GB-ThomasNeural\n",
      "en-GB-MiaNeural\n",
      "en-HK-YanNeural\n",
      "en-HK-SamNeural\n",
      "en-IE-EmilyNeural\n",
      "en-IE-ConnorNeural\n",
      "en-IN-AaravNeural\n",
      "en-IN-AashiNeural\n",
      "en-IN-AnanyaNeural\n",
      "en-IN-KavyaNeural\n",
      "en-IN-KunalNeural\n",
      "en-IN-NeerjaNeural\n",
      "en-IN-PrabhatNeural\n",
      "en-IN-RehaanNeural\n",
      "en-KE-AsiliaNeural\n",
      "en-KE-ChilembaNeural\n",
      "en-NG-EzinneNeural\n",
      "en-NG-AbeoNeural\n",
      "en-NZ-MollyNeural\n",
      "en-NZ-MitchellNeural\n",
      "en-PH-RosaNeural\n",
      "en-PH-JamesNeural\n",
      "en-SG-LunaNeural\n",
      "en-SG-WayneNeural\n",
      "en-TZ-ImaniNeural\n",
      "en-TZ-ElimuNeural\n",
      "en-US-AvaMultilingualNeural\n",
      "en-US-AndrewMultilingualNeural\n",
      "en-US-EmmaMultilingualNeural\n",
      "en-US-BrianMultilingualNeural\n",
      "en-US-AvaNeural\n",
      "en-US-AndrewNeural\n",
      "en-US-EmmaNeural\n",
      "en-US-BrianNeural\n",
      "en-US-JennyNeural\n",
      "en-US-GuyNeural\n",
      "en-US-AriaNeural\n",
      "en-US-DavisNeural\n",
      "en-US-JaneNeural\n",
      "en-US-JasonNeural\n",
      "en-US-KaiNeural\n",
      "en-US-LunaNeural\n",
      "en-US-SaraNeural\n",
      "en-US-TonyNeural\n",
      "en-US-NancyNeural\n",
      "en-US-CoraMultilingualNeural\n",
      "en-US-ChristopherMultilingualNeural\n",
      "en-US-BrandonMultilingualNeural\n",
      "en-US-AmberNeural\n",
      "en-US-AnaNeural\n",
      "en-US-AshleyNeural\n",
      "en-US-BrandonNeural\n",
      "en-US-ChristopherNeural\n",
      "en-US-CoraNeural\n",
      "en-US-ElizabethNeural\n",
      "en-US-EricNeural\n",
      "en-US-JacobNeural\n",
      "en-US-JennyMultilingualNeural\n",
      "en-US-MichelleNeural\n",
      "en-US-MonicaNeural\n",
      "en-US-RogerNeural\n",
      "en-US-RyanMultilingualNeural\n",
      "en-US-SteffanNeural\n",
      "en-ZA-LeahNeural\n",
      "en-ZA-LukeNeural\n"
     ]
    }
   ],
   "source": [
    "voices_result = speech_synthesizer.get_voices_async().get()\n",
    "\n",
    "# iterate through the list of voices\n",
    "for name in [v.short_name for v in voices_result.voices]:\n",
    "\n",
    "    # Print English voices only\n",
    "    if name[:2] == 'en':\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, it's nice to see you this morning. Are you coming to the meeting at 8:30 later?\"\n",
    "\n",
    "# Specify the voice name\n",
    "speech_config.speech_synthesis_voice_name = \"en-GB-AdaMultilingualNeural\"\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "\n",
    "# When executing this code, the speaker will be used to speak the text out loud\n",
    "result = speech_synthesizer.speak_text_async(text).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech-to-Text\n",
    "- [Example notebook - with microphone](https://github.com/Azure-Samples/cognitive-services-speech-sdk/blob/master/quickstart/python/from-microphone/quickstart.ipynb)\n",
    "\n",
    "Starts speech recognition, and returns after a single utterance is recognized.\n",
    "\n",
    "The end of a single utterance is determined by listening for silence at the end or until a maximum of about 30 seconds of audio is processed. <br/>\n",
    "The task returns the recognition text as result.\n",
    "\n",
    "Note: Since `recognize_once()` returns only a single utterance, it is suitable only for single shot recognition like command or query. For long-running multi-utterance recognition, use `start_continuous_recognition()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n",
    "result = speech_recognizer.recognize_once()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Read this sentence`\n",
    "\n",
    "\"Good morning everyone, I am thrilled to be here and excited to be speaking to you today.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized: Good morning, everyone. I'm thrilled to be here and excited to be speaking to you today.\n"
     ]
    }
   ],
   "source": [
    "if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(\"Recognized: {}\".format(result.text))\n",
    "elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"No speech could be recognized: {}\".format(result.no_match_details))\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(cancellation_details.error_details))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
